{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eebf81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#--- NLP ---\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Topic Modeling (scikit-learn) ---\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "# Download required NLTK data (runs once; safe to re-run)\n",
    "nltk.download('punkt',        quiet=True)\n",
    "nltk.download('punkt_tab',    quiet=True)\n",
    "nltk.download('stopwords',    quiet=True)\n",
    "nltk.download('wordnet',      quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45ae0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "# Paths are relative to the notebook location inside '2. Topic_modelling/'\n",
    "DATA_DIR  = \"../1.Collection_midterm/ucsb_scraping/data/raw_texts/\"\n",
    "META_PATH = \"../1.Collection_midterm/ucsb_scraping/data/metadata.csv\"\n",
    "\n",
    "K_VALUES     = [5, 10, 15]   # number of topics to evaluate\n",
    "TOP_N        = 10            # top words to display per topic\n",
    "RANDOM_STATE = 42            # seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7d077",
   "metadata": {},
   "source": [
    "# Step 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5799af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded: 485 rows\n",
      "      filename       date                                              title  \\\n",
      "0  doc_000.txt  18-Aug-80  Address to the Veterans of Foreign Wars Conven...   \n",
      "1  doc_001.txt  20-Jan-81                       Ronald Reagan Event Timeline   \n",
      "2  doc_002.txt   3-Mar-81  Excerpts From an Interview With Walter Cronkit...   \n",
      "\n",
      "       president                                                url  \\\n",
      "0  Ronald Reagan  https://www.presidency.ucsb.edu/documents/addr...   \n",
      "1  Ronald Reagan  https://www.presidency.ucsb.edu/documents/rona...   \n",
      "2  Ronald Reagan  https://www.presidency.ucsb.edu/documents/exce...   \n",
      "\n",
      "                    source  \n",
      "0  UCSB Presidency Project  \n",
      "1  UCSB Presidency Project  \n",
      "2  UCSB Presidency Project  \n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 4: Preprocessing Pipeline\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ── 1. Load metadata ─────────────────────────────────────────────────────────\n",
    "metadata = pd.read_csv(META_PATH)\n",
    "print(f\"Metadata loaded: {len(metadata)} rows\")\n",
    "print(metadata.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41b5553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text files found: 485\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load raw texts in filename order\n",
    "filenames = []\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.endswith('.txt'):\n",
    "        filenames.append(f)\n",
    "print(f\"\\nText files found: {len(filenames)}\")\n",
    "\n",
    "raw_texts = {}\n",
    "for fname in filenames:\n",
    "    with open(os.path.join(DATA_DIR, fname), 'r', encoding='utf-8', errors='replace') as fh:\n",
    "        raw_texts[fname] = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bbf37e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents preprocessed : 485\n",
      "Total tokens           : 520,846\n",
      "Average tokens/doc     : 1074\n",
      "\n",
      "Sample tokens from doc_000:\n",
      "  ['iran', 'arm', 'contra', 'aid', 'controversy', 'president', 'good', 'evening', 'word', 'take', 'question', 'brief', 'remark', 'eighteen', 'month', 'ago', 'said', 'last', 'thursday', 'administration']\n"
     ]
    }
   ],
   "source": [
    "# 3. Define stopwords \n",
    "standard_stops = set(stopwords.words('english')) # using nltk function \n",
    "\n",
    "CUSTOM_STOPWORDS = {\n",
    "    \"america\", \"american\", \"americans\", \"people\", \"nation\", \"government\",\n",
    "    \"president\", \"reagan\", \"united\", \"states\", \"would\", \"also\", \"said\",\n",
    "    \"mr\", \"mrs\", \"secretary\", \"administration\", \"year\", \"years\",\n",
    "    \"country\"\n",
    "}\n",
    "\n",
    "all_stops = standard_stops | CUSTOM_STOPWORDS\n",
    "\n",
    "# Preprocessing function \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    #clean one doc and return list of processed tokens\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)   # remove [Applause], [Laughter], etc.\n",
    "    text = text.lower()                     # lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)  # keep only letters\n",
    "    tokens = word_tokenize(text)            # split into words\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in standard_stops and len(tok) >= 3\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# ── 5. Apply preprocessing to all documents ──────────────────────────────────\n",
    "tokenized_docs  = []\n",
    "processed_texts = []\n",
    "doc_filenames   = []\n",
    "\n",
    "for fname in filenames:\n",
    "    tokens = preprocess(raw_texts[fname])\n",
    "    tokenized_docs.append(tokens)\n",
    "    processed_texts.append(\" \".join(tokens))\n",
    "    doc_filenames.append(fname)\n",
    "\n",
    "# ── 6. Sanity checks ──────────────────────────────────────────────────────────\n",
    "total_tokens = sum(len(d) for d in tokenized_docs)\n",
    "avg_tokens   = total_tokens / len(tokenized_docs)\n",
    "\n",
    "print(f\"\\nDocuments preprocessed : {len(processed_texts)}\")\n",
    "print(f\"Total tokens           : {total_tokens:,}\")\n",
    "print(f\"Average tokens/doc     : {avg_tokens:.0f}\")\n",
    "print(f\"\\nSample tokens from doc_000:\\n  {tokenized_docs[0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38c90f",
   "metadata": {},
   "source": [
    "# Step 2: Vectorization \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsb_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
