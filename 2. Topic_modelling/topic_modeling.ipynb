{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling: Reagan's Nicaragua Rhetoric (1980–1988)\n",
    "\n",
    "**Research Question:** What are the latent themes in Reagan's Nicaragua discourse, and what is the relative weight of a *counternarcotics* framing versus an *anticommunism/counterinsurgency* framing?\n",
    "\n",
    "**Corpus:** 485 speeches, statements, and remarks by Ronald Reagan containing the word "Nicaragua," scraped from the [UCSB American Presidency Project](https://www.presidency.ucsb.edu/). Documents span August 1980 through December 1988.\n",
    "\n",
    "**Approach:** We train two topic models — **LDA** (Latent Dirichlet Allocation) and **NMF** (Non-negative Matrix Factorization) — across three values of k (5, 10, 15 topics). We compare their coherence scores to assess topic quality, display the top words for each topic, and produce a document-topic distribution table mapping each speech to its dominant theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 2: Imports & Configuration\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# --- NLP ---\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Topic Modeling (scikit-learn) ---\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "# --- Coherence Scoring (gensim) ---\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Download required NLTK data (runs once; safe to re-run)\n",
    "nltk.download('punkt',        quiet=True)\n",
    "nltk.download('punkt_tab',    quiet=True)\n",
    "nltk.download('stopwords',    quiet=True)\n",
    "nltk.download('wordnet',      quiet=True)\n",
    "\n",
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "# Paths are relative to the notebook location inside '2. Topic_modelling/'\n",
    "DATA_DIR  = \"../1.Collection_midterm/ucsb_scraping/data/raw_texts/\"\n",
    "META_PATH = \"../1.Collection_midterm/ucsb_scraping/data/metadata.csv\"\n",
    "\n",
    "K_VALUES     = [5, 10, 15]   # number of topics to evaluate\n",
    "TOP_N        = 10            # top words to display per topic\n",
    "RANDOM_STATE = 42            # seed for reproducibility\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Preprocessing\n",
    "\n",
    "Before feeding text to a topic model, we need to clean and standardize it. This step is conceptually identical to preprocessing in R (e.g., with `quanteda` or `tm`), but done with Python's `nltk` and `re` libraries.\n",
    "\n",
    "**Pipeline:**\n",
    "1. **Remove bracketed stage directions** — The UCSB transcripts include audience cues like `[Applause]`, `[Laughter]`, and `[Q—]`. These are metadata about the delivery context, not substantive rhetorical content, so we strip them with a regex.\n",
    "2. **Lowercase** — Ensures "Soviet" and "soviet" are treated as the same token.\n",
    "3. **Remove punctuation and digits** — Keep only alphabetic characters.\n",
    "4. **Tokenize** — Split text into a list of individual words (tokens).\n",
    "5. **Remove stopwords** — Two layers:\n",
    "   - *Standard English stopwords* (NLTK): "the", "is", "at", etc.\n",
    "   - *Custom domain stopwords*: High-frequency Reagan-speech words that appear across nearly all documents and add noise rather than signal (e.g., "america", "president", "government"). Critically, "nicaragua" and "nicaraguan" are also removed — because this was the search keyword, they appear in virtually every document and cannot discriminate between topics.\n",
    "6. **Lemmatize** — Reduce words to their base form ("fighting" → "fight", "soviets" → "soviet"). In R this is analogous to `dfm_wordstem()`.\n",
    "7. **Filter short tokens** — Drop any token with fewer than 3 characters (removes noise like "uh", "ok")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 4: Preprocessing Pipeline\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ── 1. Load metadata ─────────────────────────────────────────────────────────\n",
    "# pandas.read_csv() is the Python equivalent of read.csv() in R.\n",
    "metadata = pd.read_csv(META_PATH)\n",
    "print(f\"Metadata loaded: {len(metadata)} rows\")\n",
    "print(metadata.head(3))\n",
    "\n",
    "# ── 2. Load raw texts in filename order (matches metadata row order) ──────────\n",
    "# We sort filenames so doc_000 < doc_001 < ... < doc_484\n",
    "filenames = sorted(\n",
    "    [f for f in os.listdir(DATA_DIR) if f.endswith('.txt')]\n",
    ")\n",
    "print(f\"\\nText files found: {len(filenames)}\")\n",
    "\n",
    "raw_texts = {}\n",
    "for fname in filenames:\n",
    "    with open(os.path.join(DATA_DIR, fname), 'r', encoding='utf-8', errors='replace') as fh:\n",
    "        raw_texts[fname] = fh.read()\n",
    "\n",
    "# ── 3. Define stopwords ───────────────────────────────────────────────────────\n",
    "standard_stops = set(stopwords.words('english'))\n",
    "\n",
    "# Custom stopwords: high-frequency but low-information words in this corpus.\n",
    "# These are so common across all Reagan speeches that they cannot distinguish\n",
    "# one topic from another — removing them lets the model surface real themes.\n",
    "CUSTOM_STOPWORDS = {\n",
    "    \"america\", \"american\", \"americans\", \"people\", \"nation\", \"government\",\n",
    "    \"president\", \"reagan\", \"united\", \"states\", \"would\", \"also\", \"said\",\n",
    "    \"mr\", \"mrs\", \"secretary\", \"administration\", \"year\", \"years\",\n",
    "    \"country\", \"world\", \"think\", \"know\", \"want\", \"get\", \"let\", \"make\",\n",
    "    \"well\", \"going\", \"today\", \"new\", \"one\", \"time\", \"come\", \"back\",\n",
    "    \"nicaragua\", \"nicaraguan\",   # the search keyword — present in all 485 docs\n",
    "    \"congress\", \"house\", \"senate\", \"vote\", \"law\", \"policy\", \"support\",\n",
    "    \"must\", \"us\", \"every\", \"even\", \"still\", \"last\", \"first\", \"two\",\n",
    "    \"ask\", \"say\", \"see\", \"take\", \"right\", \"way\", \"work\"\n",
    "}\n",
    "\n",
    "all_stops = standard_stops | CUSTOM_STOPWORDS\n",
    "\n",
    "# ── 4. Preprocessing function ─────────────────────────────────────────────────\n",
    "# In Python, defining a reusable function with `def` is like writing a function\n",
    "# in R. We'll apply this function to every document.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Clean one document and return a list of processed tokens.\"\"\"\n",
    "    # Step A: remove bracketed stage directions (e.g. [Applause], [Q—])\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    # Step B: lowercase\n",
    "    text = text.lower()\n",
    "    # Step C: keep only letters and whitespace\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # Step D: tokenize into individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Steps E–G: filter, lemmatize, drop short tokens\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in all_stops and len(tok) >= 3\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# ── 5. Apply preprocessing to all documents ──────────────────────────────────\n",
    "# We iterate over filenames (sorted) so the order matches the metadata DataFrame.\n",
    "# `tokenized_docs` is a list of token lists (needed for gensim coherence scoring).\n",
    "# `processed_texts` is a list of joined strings (needed for sklearn vectorizers).\n",
    "\n",
    "tokenized_docs  = []   # list of lists: [[\"soviet\", \"weapon\", ...], [\"drug\", ...], ...]\n",
    "processed_texts = []   # list of strings: [\"soviet weapon ...\", \"drug ...\", ...]\n",
    "doc_filenames   = []   # parallel list of filenames (for merging with metadata later)\n",
    "\n",
    "for fname in filenames:\n",
    "    tokens = preprocess(raw_texts[fname])\n",
    "    tokenized_docs.append(tokens)\n",
    "    processed_texts.append(\" \".join(tokens))\n",
    "    doc_filenames.append(fname)\n",
    "\n",
    "# ── 6. Sanity checks ──────────────────────────────────────────────────────────\n",
    "total_tokens = sum(len(d) for d in tokenized_docs)\n",
    "avg_tokens   = total_tokens / len(tokenized_docs)\n",
    "\n",
    "print(f\"\\nDocuments preprocessed : {len(processed_texts)}\")\n",
    "print(f\"Total tokens           : {total_tokens:,}\")\n",
    "print(f\"Average tokens/doc     : {avg_tokens:.0f}\")\n",
    "print(f\"\\nSample tokens from doc_000:\\n  {tokenized_docs[0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Vectorization\n",
    "\n",
    "Topic models don't work with raw text — they work with a **document-term matrix (DTM)**: a table where each row is a document, each column is a word in the vocabulary, and each cell is a number representing how important that word is in that document. This is equivalent to `quanteda::dfm()` in R.\n",
    "\n",
    "We create **two** versions of the DTM, because our two algorithms have different requirements:\n",
    "\n",
    "| Vectorizer | Values | Used for | Why |\n",
    "|---|---|---|---|\n",
    "| `CountVectorizer` | Raw word counts (integers) | LDA | LDA is a probabilistic model that assumes documents are generated by sampling words from topic distributions — it needs actual counts |\n",
    "| `TfidfVectorizer` | TF-IDF scores (decimals) | NMF | NMF is algebraic; TF-IDF downweights globally common words, giving NMF cleaner signal |\n",
    "\n",
    "**Key parameters:**\n",
    "- `max_df=0.95` — ignore terms that appear in more than 95% of documents (they are too common to discriminate between topics)\n",
    "- `min_df=3` — ignore terms that appear in fewer than 3 documents (too rare to be meaningful)\n",
    "- `max_features=5000` — cap the vocabulary size to the 5,000 most frequent terms (reduces noise, speeds up training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 6: Vectorization\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# CountVectorizer for LDA — raw integer word counts\n",
    "count_vec = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=3,\n",
    "    max_features=5000\n",
    ")\n",
    "dtm_count = count_vec.fit_transform(processed_texts)\n",
    "# fit_transform() does two things in one step:\n",
    "#   1. fit()      → learns the vocabulary from the corpus\n",
    "#   2. transform()→ converts each document to a vector of word counts\n",
    "\n",
    "# TfidfVectorizer for NMF — TF-IDF weighted scores\n",
    "tfidf_vec = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=3,\n",
    "    max_features=5000\n",
    ")\n",
    "dtm_tfidf = tfidf_vec.fit_transform(processed_texts)\n",
    "\n",
    "print(f\"DTM shape (count) : {dtm_count.shape}  ← (documents × vocabulary terms)\")\n",
    "print(f\"DTM shape (TF-IDF): {dtm_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Topic Modeling Algorithms\n",
    "\n",
    "### LDA — Latent Dirichlet Allocation\n",
    "LDA is a **generative probabilistic model**: it imagines that each document was "written" by first sampling a mixture of topics (e.g., 70% anticommunism, 20% contra aid, 10% diplomacy), and then generating each word by sampling from the word distribution of the chosen topic. Training the model means inferring those mixtures from the observed word counts.\n",
    "\n",
    "Think of it as soft clustering: documents aren't assigned to a single topic — they have a *distribution* over all topics.\n",
    "\n",
    "### NMF — Non-negative Matrix Factorization\n",
    "NMF takes a different approach: it **factorizes** the TF-IDF document-term matrix `V` into two smaller matrices:\n",
    "- `W` (documents × topics): how much of each topic is in each document\n",
    "- `H` (topics × words): which words define each topic\n",
    "\n",
    "NMF is more algebraic than probabilistic, but often produces **sharper, more coherent topics** for short-to-medium corpora. The non-negativity constraint (all values ≥ 0) encourages part-based, additive representations.\n",
    "\n",
    "### Evaluating k with Coherence Scores\n",
    "How do we know if k=5, 10, or 15 topics is the right number? We use **coherence scores (c_v metric)**, which measure whether the top words in each topic frequently co-occur in the corpus. Higher coherence = more semantically meaningful topics.\n",
    "\n",
    "> *Note:* You may also see **perplexity** used to evaluate LDA. Perplexity measures how well the model predicts held-out documents (statistical fit), but it correlates poorly with human judgments of topic quality. Coherence correlates much better with what humans actually find interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 9: Train All Models (k = 5, 10, 15) and Compute Coherence Scores\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# gensim's CoherenceModel needs the corpus as a gensim Dictionary + token lists.\n",
    "# We build the Dictionary once here (it maps each unique word to an integer ID).\n",
    "gensim_dict = Dictionary(tokenized_docs)\n",
    "\n",
    "def get_top_words(model, vectorizer, n=TOP_N):\n",
    "    \"\"\"Return top n words for each topic as a list of lists.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # model.components_ is the topic-word matrix (shape: n_topics × vocab_size)\n",
    "    # For each topic (row), we argsort the weights and take the top n indices.\n",
    "    topics = []\n",
    "    for topic_weights in model.components_:\n",
    "        top_indices = topic_weights.argsort()[:-n - 1:-1]   # descending order\n",
    "        topics.append([feature_names[i] for i in top_indices])\n",
    "    return topics\n",
    "\n",
    "def compute_coherence(top_words_list):\n",
    "    \"\"\"Compute c_v coherence score for a list of topic word lists.\"\"\"\n",
    "    cm = CoherenceModel(\n",
    "        topics=top_words_list,\n",
    "        texts=tokenized_docs,\n",
    "        dictionary=gensim_dict,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    return cm.get_coherence()\n",
    "\n",
    "# ── Train all models ──────────────────────────────────────────────────────────\n",
    "# We store every trained model and its coherence score in a dict keyed by k.\n",
    "results = {}\n",
    "\n",
    "print(f\"{'k':>4}  {'LDA coherence':>15}  {'NMF coherence':>15}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for k in K_VALUES:\n",
    "    # --- LDA (on raw counts) ---\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=k,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=20,          # increase iterations for better convergence\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(dtm_count)\n",
    "    lda_words = get_top_words(lda, count_vec)\n",
    "    lda_coh   = compute_coherence(lda_words)\n",
    "\n",
    "    # --- NMF (on TF-IDF) ---\n",
    "    nmf = NMF(\n",
    "        n_components=k,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=400\n",
    "    )\n",
    "    nmf.fit(dtm_tfidf)\n",
    "    nmf_words = get_top_words(nmf, tfidf_vec)\n",
    "    nmf_coh   = compute_coherence(nmf_words)\n",
    "\n",
    "    results[k] = {\n",
    "        'lda': lda, 'lda_words': lda_words, 'lda_coherence': lda_coh,\n",
    "        'nmf': nmf, 'nmf_words': nmf_words, 'nmf_coherence': nmf_coh\n",
    "    }\n",
    "    print(f\"{k:>4}  {lda_coh:>15.4f}  {nmf_coh:>15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 10: Coherence Plot\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "lda_scores = [results[k]['lda_coherence'] for k in K_VALUES]\n",
    "nmf_scores = [results[k]['nmf_coherence'] for k in K_VALUES]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "ax.plot(K_VALUES, lda_scores, marker='o', linewidth=2, label='LDA (c_v)')\n",
    "ax.plot(K_VALUES, nmf_scores, marker='s', linewidth=2, label='NMF (c_v)')\n",
    "\n",
    "ax.set_xlabel('Number of Topics (k)', fontsize=12)\n",
    "ax.set_ylabel('Coherence Score (c_v)', fontsize=12)\n",
    "ax.set_title('Topic Coherence vs. Number of Topics\\nReagan Nicaragua Corpus', fontsize=13)\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(K_VALUES))\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation: A higher coherence score indicates that the top words\")\n",
    "print(\"within each topic co-occur more frequently in the corpus — i.e., the topics\")\n",
    "print(\"are more semantically coherent and human-interpretable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Inspecting Topics\n",
    "\n",
    "Below we display the **top 10 words** for every topic, for all three values of k, and for both LDA and NMF. This is the core interpretive step: looking at these word lists, we can manually assign meaningful labels to each topic (e.g., *Anticommunism*, *Counternarcotics*, *Contra Aid Debate*, *Diplomacy*, etc.).\n",
    "\n",
    "Words are ranked by their weight within each topic — the higher the weight, the more strongly that word "belongs" to that topic relative to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 12: Display Top-10 Words for All Models\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "for k in K_VALUES:\n",
    "    for model_name in ['lda', 'nmf']:\n",
    "        label    = model_name.upper()\n",
    "        vec_name = 'count' if model_name == 'lda' else 'tfidf'\n",
    "        words    = results[k][f'{model_name}_words']\n",
    "        coh      = results[k][f'{model_name}_coherence']\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  {label} | k={k}   (coherence: {coh:.4f})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for i, topic_words in enumerate(words):\n",
    "            print(f\"  Topic {i+1:>2}: {', '.join(topic_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Document-Topic Distribution\n",
    "\n",
    "The topic word lists tell us what each topic *is*. Now we want to know: for each document (each speech or statement), **which topic dominates, and how strongly?**\n",
    "\n",
    "Both LDA and NMF produce a **document-topic matrix** — a table where each row is a document and each column is a topic, and each cell contains the weight (or probability) of that topic in that document. We:\n",
    "1. Find the **dominant topic** for each document (`argmax` — the topic with the highest weight)\n",
    "2. Record the **topic weight** (how concentrated the document is on that dominant topic; closer to 1.0 = more focused)\n",
    "3. Merge with the original metadata (date, title) for interpretability\n",
    "\n",
    "This allows us to answer questions like: *"How many speeches are primarily about counternarcotics vs. anticommunism?"* or *"Did the dominant framing shift over time?"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CELL 14: Document-Topic Distribution Tables\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Ensure metadata and doc_filenames are aligned\n",
    "# We'll merge on 'filename' so order doesn't matter.\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "\n",
    "for k in K_VALUES:\n",
    "    for model_name in ['lda', 'nmf']:\n",
    "        model = results[k][model_name]\n",
    "        label = model_name.upper()\n",
    "\n",
    "        # Get the doc-topic matrix\n",
    "        # .transform() projects the documents into topic space using the\n",
    "        # already-trained model — it does NOT re-train.\n",
    "        if model_name == 'lda':\n",
    "            doc_topic = model.transform(dtm_count)\n",
    "        else:\n",
    "            doc_topic = model.transform(dtm_tfidf)\n",
    "\n",
    "        # For NMF, values are not probabilities (they don't sum to 1).\n",
    "        # We normalize each row so topic weights sum to 1 — making them\n",
    "        # comparable to LDA's probability distributions.\n",
    "        if model_name == 'nmf':\n",
    "            row_sums = doc_topic.sum(axis=1, keepdims=True)\n",
    "            # Avoid division by zero for any all-zero rows\n",
    "            row_sums[row_sums == 0] = 1\n",
    "            doc_topic = doc_topic / row_sums\n",
    "\n",
    "        # Dominant topic (0-indexed) and its weight\n",
    "        dominant_topic = np.argmax(doc_topic, axis=1) + 1   # +1 → 1-indexed\n",
    "        topic_weight   = np.max(doc_topic, axis=1)\n",
    "\n",
    "        # Build a DataFrame with filename + results\n",
    "        result_df = pd.DataFrame({\n",
    "            'filename'      : doc_filenames,\n",
    "            'dominant_topic': dominant_topic,\n",
    "            'topic_weight'  : topic_weight.round(4)\n",
    "        })\n",
    "\n",
    "        # Merge with metadata on filename\n",
    "        merged = result_df.merge(meta_df[['filename', 'date', 'title']], on='filename')\n",
    "        merged = merged[['filename', 'date', 'title', 'dominant_topic', 'topic_weight']]\n",
    "        merged = merged.sort_values('filename').reset_index(drop=True)\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"  {label} | k={k}  —  Document-Topic Distribution\")\n",
    "        print(f\"  Total documents: {len(merged)}\")\n",
    "        print(f\"  Topic distribution (document counts):\")\n",
    "        # How many documents are dominated by each topic?\n",
    "        topic_counts = merged['dominant_topic'].value_counts().sort_index()\n",
    "        for topic_id, count in topic_counts.items():\n",
    "            pct = count / len(merged) * 100\n",
    "            print(f\"    Topic {topic_id:>2}: {count:>4} docs  ({pct:.1f}%)\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Display first 20 rows as a preview table\n",
    "        display(merged.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
